{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc556ed6-deb4-4ac0-84eb-33248e6a3844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. 패키지 자동 설치 함수\n",
    "# ---------------------------------------------------------\n",
    "def install_package(module_name, package_name=None):\n",
    "    if package_name is None:\n",
    "        package_name = module_name\n",
    "    try:\n",
    "        importlib.import_module(module_name)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package_name} ...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "            print(f\"{package_name} installation completed\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"{package_name} installation failed (exit code {e.returncode})\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "# 필수 패키지 확인 및 설치\n",
    "print(\"Checking required packages...\")\n",
    "install_package(\"tensorflow\", \"tensorflow==2.10.0\")\n",
    "install_package(\"numpy\")\n",
    "install_package(\"pandas\")\n",
    "install_package(\"tqdm\")\n",
    "install_package(\"PIL\", \"Pillow\")\n",
    "install_package(\"cv2\", \"opencv-python\")\n",
    "install_package(\"matplotlib\")\n",
    "install_package(\"sklearn\", \"scikit-learn\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. 라이브러리 임포트\n",
    "# ---------------------------------------------------------\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. 데이터 로드 및 전처리\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(\"Please select the data folder window...\")\n",
    "root = tk.Tk()\n",
    "root.attributes('-topmost', True)\n",
    "root.withdraw()\n",
    "root.focus_force()\n",
    "dirname = filedialog.askdirectory(title='Select Data Folder to Train')\n",
    "root.destroy()\n",
    "\n",
    "if not dirname:\n",
    "    print(\"No folder selected. Exiting...\")\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"Selected Directory: {dirname}\")\n",
    "\n",
    "if not dirname.endswith('/') and not dirname.endswith('\\\\'):\n",
    "    dirname += os.sep\n",
    "\n",
    "def image_to_tensor(img_path):\n",
    "    try:\n",
    "        path = os.path.join(dirname, img_path)\n",
    "        if not os.path.exists(path):\n",
    "            return None\n",
    "            \n",
    "        img = keras_image.load_img(path, target_size=(120, 160))\n",
    "        x = keras_image.img_to_array(img)\n",
    "        \n",
    "        # [중요] EfficientNet 전처리 유지\n",
    "        x = preprocess_input(x) \n",
    "        \n",
    "        return np.expand_dims(x, axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "csv_path = os.path.join(dirname, \"0_road_labels.csv\")\n",
    "if not os.path.exists(csv_path):\n",
    "    print(f\"Error: CSV file '{csv_path}' not found. Creating a new empty one in memory.\")\n",
    "    data = pd.DataFrame(columns=['file', 'label'])\n",
    "else:\n",
    "    data = pd.read_csv(csv_path)\n",
    "    print(f\"CSV Loaded. Rows: {len(data)}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 폴더/파일 동기화 및 자동 라벨링\n",
    "# ---------------------------------------------------------\n",
    "print(\"Syncing CSV and File System (Recursive search)...\")\n",
    "\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "real_files_in_folder = set()\n",
    "\n",
    "# 하위 폴더까지 탐색\n",
    "for root_dir, dirs, files in os.walk(dirname):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(valid_extensions):\n",
    "            full_path = os.path.join(root_dir, file)\n",
    "            rel_path = os.path.relpath(full_path, dirname)\n",
    "            rel_path = rel_path.replace(\"\\\\\", \"/\")\n",
    "            real_files_in_folder.add(rel_path)\n",
    "\n",
    "files_in_csv = set(data['file'].unique())\n",
    "is_changed = False\n",
    "\n",
    "missing_files = files_in_csv - real_files_in_folder\n",
    "if len(missing_files) > 0:\n",
    "    print(f\"Warning: {len(missing_files)} files listed in CSV but missing on disk. Removing...\")\n",
    "    data = data[~data['file'].isin(missing_files)]\n",
    "    is_changed = True\n",
    "\n",
    "new_files = real_files_in_folder - files_in_csv\n",
    "if len(new_files) > 0:\n",
    "    print(f\"Info: {len(new_files)} new images found. Auto-labeling based on folder/file names...\")\n",
    "    \n",
    "    new_file_list = []\n",
    "    new_label_list = []\n",
    "    \n",
    "    for filename in new_files:\n",
    "        label = 3 # 기본값 (Stop)\n",
    "        path_lower = filename.lower()\n",
    "        \n",
    "        # 경로명(폴더명 포함)에서 키워드 검색\n",
    "        if '_0_forward' in path_lower:\n",
    "            label = 0\n",
    "        elif '_1_right' in path_lower:\n",
    "            label = 1\n",
    "        elif '_2_left' in path_lower:\n",
    "            label = 2\n",
    "        elif '_3_stop' in path_lower:\n",
    "            label = 3\n",
    "        elif '_4_backward' in path_lower:\n",
    "            label = 4\n",
    "        \n",
    "        new_file_list.append(filename)\n",
    "        new_label_list.append(label)\n",
    "        \n",
    "    new_entries = pd.DataFrame({'file': new_file_list, 'label': new_label_list})\n",
    "    data = pd.concat([data, new_entries], ignore_index=True)\n",
    "    is_changed = True\n",
    "    print(f\"Added {len(new_entries)} files.\")\n",
    "\n",
    "if is_changed:\n",
    "    data.to_csv(csv_path, index=False)\n",
    "    print(f\"Successfully overwrote CSV file: {csv_path}\")\n",
    "else:\n",
    "    print(\"No changes needed for CSV.\")\n",
    "\n",
    "# 필터링 (후진 데이터 제거)\n",
    "data = data[data['label'] != 4]\n",
    "data = data.reset_index(drop=True)\n",
    "print(f\"Final data count for training: {len(data)}\")\n",
    "\n",
    "print(\"Loading images into tensors...\")\n",
    "\n",
    "valid_tensors = []\n",
    "valid_labels = []\n",
    "valid_files = [] \n",
    "\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    img_path = row['file']\n",
    "    label = row['label']\n",
    "    \n",
    "    tensor = image_to_tensor(img_path)\n",
    "    \n",
    "    if tensor is not None:\n",
    "        valid_tensors.append(tensor)\n",
    "        valid_labels.append(label)\n",
    "        valid_files.append(img_path)\n",
    "\n",
    "if len(valid_tensors) > 0:\n",
    "    tensors = np.vstack(valid_tensors)\n",
    "    targets = np.array(valid_labels)\n",
    "    files = np.array(valid_files)\n",
    "else:\n",
    "    print(\"Error: No valid images loaded.\")\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"Final loaded tensors shape: {tensors.shape}\")\n",
    "print(f\"Final loaded targets shape: {targets.shape}\")\n",
    "\n",
    "if tensors.shape[0] != targets.shape[0]:\n",
    "    print(\"Error: Mismatch between images and labels.\")\n",
    "    sys.exit()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. 데이터 시각화\n",
    "# ---------------------------------------------------------\n",
    "names = ['_0_forward', '_1_right', '_2_left', '_3_stop']\n",
    "\n",
    "def display_images(img_path, ax):\n",
    "    full_path = os.path.join(dirname, img_path)\n",
    "    if os.path.exists(full_path):\n",
    "        img = cv2.imread(full_path)\n",
    "        if img is not None:\n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Corrupt\", ha='center', va='center')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"Missing\", ha='center', va='center')\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "count = 0\n",
    "for i in range(len(files)):\n",
    "    if count >= 4: break\n",
    "    ax = fig.add_subplot(1, 4, count + 1, xticks=[], yticks=[])\n",
    "    ax.set_title(names[targets[i]], color='blue')\n",
    "    display_images(files[i], ax)\n",
    "    count += 1\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. 데이터셋 분리\n",
    "# ---------------------------------------------------------\n",
    "tensors = tensors.reshape(-1, 120, 160, 3)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "unique_classes, counts = np.unique(targets, return_counts=True)\n",
    "\n",
    "MIN_COUNT = BATCH_SIZE \n",
    "\n",
    "insufficient_classes = unique_classes[counts < MIN_COUNT]\n",
    "\n",
    "if len(insufficient_classes) > 0:\n",
    "    print(f\"\\n[경고] 데이터가 {MIN_COUNT}개 미만인 클래스(방향)를 학습에서 제외합니다: {insufficient_classes}\")\n",
    "    \n",
    "    mask = ~np.isin(targets, insufficient_classes)\n",
    "    \n",
    "    tensors = tensors[mask]\n",
    "    targets = targets[mask]\n",
    "    \n",
    "    print(f\"필터링 후 남은 데이터 개수: {len(targets)}\")\n",
    "else:\n",
    "    print(f\"\\n모든 클래스가 {MIN_COUNT}개 이상의 데이터를 가지고 있습니다. 필터링 없음.\")\n",
    "\n",
    "if len(targets) == 0:\n",
    "    print(\"Error: 모든 데이터가 기준 미달로 제거되었습니다.\")\n",
    "    sys.exit()\n",
    "# =========================================================\n",
    "\n",
    "targets = to_categorical(targets, 4)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "        tensors,\n",
    "        targets,\n",
    "        test_size = 0.2,\n",
    "        random_state = 1,\n",
    "        stratify=targets)\n",
    "\n",
    "n = int(len(x_test)/2)\n",
    "x_valid, y_valid = x_test[:n], y_test[:n]\n",
    "x_test, y_test = x_test[n:], y_test[n:]\n",
    "\n",
    "print(f\"Train shapes: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test shapes: {x_test.shape}, {y_test.shape}\")\n",
    "print(f\"Valid shapes: {x_valid.shape}, {y_valid.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. 모델 정의 (EfficientNetB0)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Building EfficientNetB0 Model...\")\n",
    "\n",
    "# 1. Base Model 로드\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(120, 160, 3))\n",
    "\n",
    "base_model.trainable = True\n",
    "\n",
    "fine_tune_at = -60\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in base_model.layers[fine_tune_at:]:\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = False\n",
    "\n",
    "inputs = Input(shape=(120, 160, 3))\n",
    "x = base_model(inputs)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(128, activation='relu')(x) \n",
    "x = Dropout(0.5)(x) \n",
    "\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "outputs = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. 학습 실행\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# [설정 1] 목표 정확도 도달 시 종료\n",
    "class ReachTargetAccuracy(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, target_acc):\n",
    "        super(ReachTargetAccuracy, self).__init__()\n",
    "        self.target_acc = target_acc\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_acc = logs.get('val_accuracy')\n",
    "        if val_acc is not None and val_acc >= self.target_acc:\n",
    "            print(f\"\\n\\n검증 정확도가 목표치({self.target_acc*100:.2f}%)에 도달했습니다: {val_acc*100:.4f}%\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# [설정 2] Loss가 더 이상 안 줄어들면 종료\n",
    "early_stop_loss = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# [설정 3] Accuracy가 더 이상 안 오르면 종료\n",
    "early_stop_acc = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# [설정 4] 학습이 정체되면 학습률을 낮춤\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5,       \n",
    "    patience=5,      \n",
    "    verbose=1,       \n",
    "    min_lr=1e-7      \n",
    ")\n",
    "\n",
    "# 목표 정확도 설정 (예: 90%)\n",
    "TARGET_ACCURACY = 0.93\n",
    "target_acc_callback = ReachTargetAccuracy(target_acc=TARGET_ACCURACY)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "callbacks_list = [early_stop_loss, early_stop_acc, target_acc_callback, reduce_lr]\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=3,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    brightness_range=[0.9, 1.1],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "steps = math.ceil(len(x_train) / BATCH_SIZE)\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    steps_per_epoch=steps,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "loss = history.history['loss']\n",
    "epochs_range = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, loss, 'g', label='Training loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(epochs_range, history.history['val_loss'], 'r', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, history.history['accuracy'], 'b', label='Accuracy')\n",
    "if 'val_accuracy' in history.history:\n",
    "    plt.plot(epochs_range, history.history['val_accuracy'], 'orange', label='Val Accuracy')\n",
    "    plt.axhline(y=TARGET_ACCURACY, color='k', linestyle='--', label='Target')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 모델 저장\n",
    "# ---------------------------------------------------------\n",
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "filename_weights = f\"weights_EN_{current_time}.h5\"\n",
    "\n",
    "print(f\"Saving weights to {filename_weights}...\")\n",
    "\n",
    "try:\n",
    "    model.save_weights(filename_weights)\n",
    "    print(f\"Success! Weights saved to {filename_weights}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving weights: {e}\")\n",
    "    \n",
    "# ---------------------------------------------------------\n",
    "# 8. 테스트 및 검증\n",
    "# ---------------------------------------------------------\n",
    "print(\"Evaluating model...\")\n",
    "def create_model():\n",
    "    base_model = EfficientNetB0(weights=None, include_top=False, input_shape=(120, 160, 3))\n",
    "    fine_tune_at = -60\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[fine_tune_at:]:\n",
    "        if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "            \n",
    "    inputs = Input(shape=(120, 160, 3))\n",
    "    x = base_model(inputs)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(4, activation='softmax')(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "model_test = create_model()\n",
    "\n",
    "try:\n",
    "    print(f\"Loading weights from: {filename_weights}\")\n",
    "    model_test.load_weights(filename_weights)\n",
    "    print(\"Model loaded successfully.\")\n",
    "    \n",
    "    # 3. 예측 진행\n",
    "    y_test_predict = model_test.predict(x_test)\n",
    "    y_test_predict = np.argmax(y_test_predict, axis=1)\n",
    "\n",
    "    sample_size = min(16, x_test.shape[0])\n",
    "    if sample_size > 0:\n",
    "        fig = plt.figure(figsize=(18, 18))\n",
    "        indices = np.random.choice(x_test.shape[0], size=sample_size, replace=False)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n",
    "            img_display = x_test[idx]\n",
    "            img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())\n",
    "            \n",
    "            ax.imshow(np.squeeze(img_display))\n",
    "            pred_idx = y_test_predict[idx]\n",
    "            true_idx = np.argmax(y_test[idx])\n",
    "            ax.set_title(\"Pred:{} (True:{})\".format(names[pred_idx], names[true_idx]),\n",
    "                         color=(\"#4876ff\" if pred_idx == true_idx else \"darkred\"))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Not enough test data to visualize.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model for testing: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
